{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll express our the components of our decision tree algorithm in terms of mathematical formulas.  To do so we'll have to define a couple of terms.\n",
    "\n",
    "We'll define:\n",
    "* $F(x)$ to be hypothesis function for the decision tree \n",
    "* $R_j$ to represent a specific leaf node, or terminal region\n",
    "* $\\gamma$ to represent the prediction of that leaf node\n",
    "\n",
    "A lot of understanding the expressions that define our decision tree is simply an exercise in keeping track of these terms.  Don't worry we'll go through them as again, as we move through the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a formal definition of the hypothesis function for decision trees.  Now for a decision tree, the predictions come from each leaf, also called a terminal region.  \n",
    "\n",
    "We label each leaf, or terminal region $R_j$.  Where $j$ is an index going from $1$ to the number of leaves $J$.  That prediction for each terminal region, $R_j$,  is $\\gamma$ (gamma).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so if we wish to represent the predictions for each leaf, this is the value, $\\gamma$, that minimizes the sum of the losses of each observation that falls into that leaf.  Or to place this in an equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F(x) = \\gamma_j I(x \\in R_j) = \\underset{\\gamma}{\\mathrm{argmin}} \\sum_{x_i \\in R_{j}} L(y_i, \\gamma) I(x \\in R_j)$\n",
    "* And this is just the mean of the labels that fall in the terminal leaf nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we let $x$ represent the observations that fall into a specific node, then the loss for that node is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J_R = \\sum_{i= 1}^n(y_i - \\hat{y})^2$\n",
    "\n",
    "> Or in other words, the loss equals the variance of that node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the loss function for the entire decision tree is simply SSE, which is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(x) = \\sum_{i = 1}^n(y_i - F(x_i))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We use a greedy approach to incrementally build the tree (i.e., minimize the gini coefficient) one node at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $j = 1 ...J$\n",
    "\n",
    "> $R_{jt} = \\underset{\\theta}{\\mathrm{argmin}}  G(\\theta, x \\in R_{jt+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $G = \\frac{A}{A + B}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where A and B, can be seen by the regions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./gini-img.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components of a Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the hypothesis function of a random forest is just the average of the predictions of a set of a decision trees, which this time we will call $f_m$, where $m$ is an index for a specific estimator, or decision tree. So then the hypothesis function for our random forest is: \n",
    "\n",
    "$F(x) = \\frac{1}{M}\\sum_{i = 1}^M f_m(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the loss function of a random forest is once again the sum of the squared errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J = \\sum_{i=1}^n (F(x_i) -  y_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we covered the hypothesis function, loss function and optimization procedure of a decision tree.  The decision tree makes a separate prediction for each leaf in the tree, where the prediction of a leaf is the value that minimizes the sum of the losses of each observation that falls into that leaf.  Or to place this in an equation, we let the a leaf equal $R_j$, and the prediction of that leaf equal $\\gamma_j$, so then the prediction is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F(x)  = \\underset{\\gamma}{\\mathrm{argmin}} \\sum_{x_i \\in R_{j}} L(y_i, \\gamma) I(x_i \\in R_j)$\n",
    "\n",
    "> Or in other words, the average of the labels that fall into that leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $\\gamma$ is the average value of $y$, $\\hat{y}$ this means that the loss for a leaf node is:\n",
    "\n",
    "$J_R = \\sum_{i= 1}^n(y_i - \\hat{y})^2$, \n",
    "> Or the variance of that node.\n",
    "\n",
    "And we train the decision tree, by choosing a split point in each leaf such that if the $theta_j$ is a split point of a leaf $R_j$:\n",
    "\n",
    "> $\\theta_j = \\underset{\\theta}{\\mathrm{argmin}}  G(\\theta, x \\in R_{jt+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theta is the split point that minimizes the gini coefficient of the observations in the resulting leaf node $R_{jt + 1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Resources\n",
    "\n",
    "[Decision Trees](http://www.datasciencecourse.org/slides/decision_trees.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Wiki Gini Coefficient](https://en.wikipedia.org/wiki/Gini_coefficient)\n",
    "\n",
    "[Gini Coefficient Notes](http://www3.nccu.edu.tw/~jthuang/Gini.pdf)\n",
    "\n",
    "[Fastai Math for Gradient Boosting](https://explained.ai/gradient-boosting/descent.html)\n",
    "\n",
    "[Statquest Math for Gradient Boosting](https://www.youtube.com/watch?v=StWY5QWMXCw&t=143s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
