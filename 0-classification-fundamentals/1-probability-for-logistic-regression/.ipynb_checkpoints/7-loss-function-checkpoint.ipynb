{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we saw how logistic regression uses the sigmoid function to assign different probabilities to events.  So it states that:\n",
    "\n",
    "$P(y = 1| x) = \\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And where $z = \\sum_{j =1}^n \\theta_j*x_j + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to write this in a different way, we can write the hypothesis function of logistic regression as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $P(Y = 1|X = x) = \\sigma(\\sum_{j=1}^n\\theta_i x_i + b) $\n",
    "\n",
    "* $P(Y = 0|X = x) = 1 - \\sigma(\\sum_{j=1}^n\\theta_i x_i + b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to train our logistic regression model, we wish to find the parameters $\\theta$ that maximize that probability defined above, for all data.  And if you think this sounds like a maximum likelihood problem, you are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the Bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the bernoulli random variable, and log likelihood, and then we'll see how it translates to train our hypothesis function above.  Recall that, with our bernoulli random variable, we rewrote PMF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $f(y = 1 | p) = p $ and \n",
    "* $f(y = 0 | p) = 1 -p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(y | p) = p^{x_i}(1 - p)^{1 - x_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And similarly, here we can rewrite our hypothesis function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $P(Y = 1|X = x) = \\sigma(\\sum_{j=1}^n\\theta_i x_i + b) $\n",
    "\n",
    "* $P(Y = 0|X = x) = 1 - \\sigma(\\sum_{j=1}^n\\theta_i x_i + b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P_\\theta(Y |X ) = \\sigma(\\sum_{j=1}^n\\theta_i x_i + b)^{y_i} * (1 - \\sigma(\\sum_{j=1}^n\\theta_i x_i + b))^{1 - y_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In other words, when $y = 1$, the probability is the first part of the equation, and when $y = 0$, the probability predicted is the second part of the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remember that to view the equation above in terms of likelihood, we consider our observation as fixed, and then our probability of seeing a datapoint depends on the particular value of $\\theta$ that we choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal will be to find the parameters, $\\theta$ that maximize the likelihood for all of our observations.  So to do that, we use the product rule of probability to get to the likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_\\theta(Y |X ) = \\prod_{i = 1}^n \\sigma(\\sum_{j=1}^n\\theta_i x_i + b)^{y_i} * (1 - \\sigma(\\sum_{j=1}^n\\theta_i x_i + b))^{1 - y_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then to make this equation easier, we apply the logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$LL_\\theta(Y |X ) = \\sum_{i = 1}^n y_i log(\\sigma(\\sum_{j=1}^n\\theta_i x_i + b)) + (1 - y_i) log(1 - \\sigma(\\sum_{j=1}^n\\theta_i x_i + b))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This this equation above is what we are trying to maximize when we train our logistic regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simplify the notation by letting $z = \\sum_{j=1}^n\\theta_i x_i + b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$LL_\\theta(Y |X ) = \\sum_{i = 1}^n y_i log(\\sigma(z_\\theta)) + (1 - y_i) log(1 - \\sigma(z_\\theta))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing our Log Likelihood Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to find the theta that maximizes the log likelihood function, we cannot simply take the derivative and set it equal to zero.  Instead, we must apply gradient ascent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we start at a point on our log likelihood function, and nudge our parameters by an amount proportional to the slope of our log likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Logistic Regression - Stanford Notes](https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/pdfs/40%20LogisticRegression.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
