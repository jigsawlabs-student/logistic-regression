{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll learn some of the different ways of evaluating the performance of the a classificartion algorithm.  We'll learn to identify different ways that things can go wrong: false positives, where we wrongly predict the event occurred.  And false negatives, where we incorrectly predict that an event did not occur.  Ok, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading our breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer_data = load_breast_cancer()\n",
    "X = pd.DataFrame(cancer_data['data'], columns = cancer_data['feature_names'])\n",
    "y = pd.Series(cancer_data['target'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=5000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver = 'lbfgs', max_iter = 5000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there are different ways that we can evaluate how well our logistic regression model fits to the data.  The first way is accuracy.  \n",
    "\n",
    "> Accuracy is the percentage of observations that were predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn returns to us the accuracy score of our model out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9440559440559441"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 94% of our observations in the test set were predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the percentage of observations predicted correctly, the next component is to discover:\n",
    "* the amount of **cancerous observations** that were correctly predicted, and\n",
    "* the amount of **benign observations** that were correctly predicted.  \n",
    "\n",
    "When our classification model predicts the event as occurring, we say that the event is **positive** and a non-event is a **negative** prediction.  This leads us to the following terms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terms to categorize *correctly* made predictions:\n",
    "\n",
    "* **True positive** - Our model predicts the event occurred, and it in fact occurred\n",
    "* **True negative** - Our model predicts the event **did not** occur, and it did not occur\n",
    "\n",
    "Terms categorize the types of mistakes from our model:\n",
    "* **False positive** The model predicts an event, but it did not occur (False alarm) \n",
    "* **False negative** The model predicts a non-event, but it did occur (Missed opportunity) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's helpful to think of a false positive as a false alarm.  Our model detected there was an event where none existed.  And a false negative as a missed opportunity.  \n",
    "> While it may be odd to think of not detecting cancer as a missed \"opportunity\", it was an opportunity to begin taking the proper next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A false positive is also called a type I error, and a false negative is called a type II error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with a Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can summarize the above measures of accuracy with a confusion matrix.  The confusion matrix breaks down each type of measurement above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./confusion-matrix.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four conditions in our confusion matrix account for each of our predicted observations.  Sklearn has a built in `confusion_matrix` function to calculate the above for a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50,  5],\n",
       "       [ 3, 85]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "conf_data = confusion_matrix(y_test, y_pred_test)\n",
    "conf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import `confusion_matrix` from the metrix library.  We then format our data into a dataframe.  Notice that we had to reorder the data, so that it matches our confusion matrix from above.  Once reformatted, it returns to us the correctly categorized events diagonally down from left to right, and the falsely categorized events diagonally down from right to left.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "conf_mtx_df = pd.DataFrame(conf_data, index = ['observed -', 'observed +'],\n",
    "                           columns = ['predicted -', 'predicted +'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observed +</th>\n",
       "      <th>observed -</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>predicted +</th>\n",
       "      <td>85</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted -</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             observed +  observed -\n",
       "predicted +          85           5\n",
       "predicted -           3          50"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mtx_df.iloc[::-1, ::-1].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for our model we can see that there 85 true positives, and 50 true negatives.  Over to the top right we see 3 false positives, and at the bottom left we see 5 false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we learned about different categorized of correctly classified events: true positives and true negatives, incorrectly classified events false positives and false negatives.  We can think of false positives as *falsely positive* or false alarms.  We can think of false negatives as *falsely negative* or false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
