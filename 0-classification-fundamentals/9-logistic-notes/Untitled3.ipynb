{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Assume that in your logistic regression model, you've scalar inputs x, and the the model outputs a probability $\\hat{y}(x)=sigma(wx+b)$ for each input sample x .If you go ahead and build a quadratic loss function in the very special case where the inputs $x$ are scalars, then your cost function becomes: $C(w,b):= \\Sigma_{x} | y(x) - \\hat{y}(x)|^2=\\Sigma_{x} | y(x) - \\sigma(wx+b)|^2$. Now if you try to apply gradient descent on it, you'll see that: $C'(w), C'(b)$ are multiples of $\\sigma'(wx+b)$. Now, sigmoid function being asymptotic, its derivative $\\sigma'(z)$ becomes almost zero when the output $\\sigma(z)$ is close to $0$ or $1$. This means: when the learning is bad, e.g. $\\sigma(wx+b) \\approx 0$, but $y(x)=1$, then $C'(w), C'(b)\\approx 0$.\n",
    "\n",
    "Now, the above situation is bad from two standpoints: (1) it makes gradient descent numerically much more expensive because even when we're far from minimizing C(w,b), we're not converging fast enough, and (2) it's counterintuitive to human learning: we learn fast when we make a big mistake.\n",
    "\n",
    "However, if you calculate the C'(w) and C'(b) for cross-entropy cost function, this problem doesn't occur, as unlike the derivatives of quadratic cost, the derivatives of cross entropy cost is not a multiple of $sigma'(wx+b)$, and hence when the logistic regression model outputs close to 0 or 1, the gradient descent doesn't necessarily slow down, hence convergence to minima happens faster. You can find the relevant discussion here: http://neuralnetworksanddeeplearning.com/chap3.html, an excellent online book I highly recommend!\n",
    "\n",
    "Besides, cross entropy cost functions are just negative log of maximum likelihood functions (MLE) used to estimate the model parameters, and in fact in the case of linear regression, minimizing the quadratic cost function is equivalent to maximizing the MLE, or equivalently, minimizing the negative log of MLE=cross entropy, with the underlying model assumption for linear regression-see P. 12 of http://cs229.stanford.edu/notes/cs229-notes1.pdf for more detail. Hence, for any machine learning model, be it classification and regression, finding the parameters by maximizing MLE (or minimizing cross entropy) has a statistical significance, whereas minimizing the quadratic cost for logistic regression doesn't have any (although it does for linear regression, as stated before).\n",
    "\n",
    "I hope it clarifies things!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
